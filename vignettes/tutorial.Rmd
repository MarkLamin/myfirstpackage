---
title: "myfirstpackage Tutorial"
author: "Mark Lamin"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Project 3: PACKAGE_NAME Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(myfirstpackage)
```
## Introduction
`myfirstpackage` is a demonstration of statistical concepts learned in the STAT 302 course in Spring 2021. This package contains various functions involving statistical inference and statistical prediction. You can install the package with the following:
```{r eval=FALSE}
devtools::install_github("https://github.com/MarkLamin/myfirstpackage")
```
```{r}
library(myfirstpackage)
```

## T test tutorial
This is a tutorial for the function `my_t_test()`. We will show three examples. In each, we use a p-value cutoff of $\alpha = 0.05$. The first hypothesis test is $$H_0: \mu=60, $$ $$H_a: \mu\neq60 $$
```{r}
my_t_test(my_gapminder$lifeExp, "two.sided", 60)
```

Since our p value is greater than $\alpha$, we fail to reject the null hypothesis. That is, the data does not appear to suggest that the mean is not equal to 60.

The second hypothesis test is $$H_0: \mu=60, $$ $$H_a: \mu<60 $$
```{r}
my_t_test(my_gapminder$lifeExp, "less", 60)
```

Since our p value is less than $\alpha$, we reject the null hypothesis. That is, the data appears to suggest that the mean might be less than 60.

The third hypothesis test is $$H_0: \mu=60, $$ $$H_a: \mu>60 $$
```{r}
my_t_test(my_gapminder$lifeExp, "greater", 60)
```

Since our p value is greater than alpha, we fail to reject the null hypothesis. That is, the data does not appear suggest that the mean is greater than 60.

## Tutorial for lm
This is a tutorial for the function `my_lm()`. Our first example will use the `my_gapminder` data set.
```{r}
my_output <- my_lm(lifeExp~gdpPercap+continent, my_gapminder)
my_output
```
We can see based on the coefficient for `gdpPercap` that for every increase in `lifeExp` by one year, `gdpPercap` increases by about 4.452704e-04 when controlling for `continent`. We can write a hypothesis test associated with this coefficient, which I will refer to as $\beta_1$ where we will again use the p-value cutoff of $\alpha=0.05$: $$H_0: \beta_1=0, $$ $$H_a: \beta_1\neq0 $$ Since the p value is less than $\alpha$, we reject the null hypothesis. That is, the data appears to suggest that there might be some correlation between gdp per capita and life expectancy.

We can also compare the exact values with the expected values from the model for Life Expectancy.
```{r, fig.width = 9}
beta <- my_output[,1]
X <- model.matrix(lifeExp~gdpPercap+continent, my_gapminder)
fitted <- X %*% beta
ggplot2::ggplot(mapping = ggplot2::aes(x = fitted, y = my_gapminder$lifeExp,
                                       color = as.factor(my_gapminder$continent))) + 
  ggplot2::geom_point() + ggplot2::theme_bw() +
  ggplot2::labs(title = "Actual vs Fitted Values",
       x = "Fitted Values",
       y = "Actual Values")
```

When we control for continent, we see that the continents can be categorized clearly with this model. However, the fitted values seem to be over precise when determining the life expectancy as the actual values have a much wider range of values than the fitted values when controlling for country. It is also possible that there is a confounding variable such as time creating the correlation.

## Tutorial for knn

```{r}
my_penguins <- my_penguins %>% na.omit
for(i in(1:10)){
  model <- my_knn_cv(my_penguins[3:6], my_penguins$species, i, 5)
  print(model[2])
  print(mean(model[[1]] != my_penguins$species))
}
```

Based on the CV misclassification rates and training misclassification rates, I would choose 1 nearest neighbor. However, in practice, I might want to choose 2 or 3 nearest neighbors as while having 1 nearest neighbor would be more accurate, having slightly more nearest neighbors might increase generalizability. This is because having 2 or 3 nearest neighbors will create a better sense of the context of the specific part of the data by considering more data points in the prediction. I would still want a relatively small number of neighbors as having too many neighbors might result in a poor prediction as the context is not specific enough.

## Tutorial for Random Forest
```{r, fig.width = 9}
the_data <- matrix(NA, nrow = 90, ncol = 2)

for(i in(1:90)){
  if(1<=i&i<=30){
    the_data[i,1] = my_rf_cv(2)
    the_data[i,2] = "02"
  }else if(31<=i&i<=60){
    the_data[i,1] = my_rf_cv(5)
    the_data[i,2] = "05"
  }else if(61<=i&i<=90){
    the_data[i,1] = my_rf_cv(10)
    the_data[i,2] = "10"
  }
}
the_data <- data.frame(the_data)
           
ggplot2::ggplot(data = the_data, mapping = ggplot2::aes(x = X2,
                                               y = as.numeric(X1))) +
  ggplot2::geom_boxplot(color = "blue") + ggplot2::theme_bw() + 
  ggplot2::labs(title = "Random Forest Error Based on Different Fold Values",
                x = "Number of Folds", y = "Cross Validation Error")

kis2 <- the_data %>% dplyr::filter(the_data$X2 == "02")
kis5 <- the_data %>% dplyr::filter(the_data$X2 == "05")
kis10 <- the_data %>% dplyr::filter(the_data$X2 == "10")

mean_column <- c(mean(as.numeric(as.vector(kis2$X1))),
                 mean(as.numeric(as.vector(kis5$X1))),
                 mean(as.numeric(as.vector(kis10$X1))))
sd_column <- c(sd(as.vector(kis2$X1)), sd(as.vector(kis5$X1)),
               sd(as.vector(kis10$X1)))

a_table <- cbind(mean_column, sd_column)
colnames(a_table) <- c("mean", "sd")
rownames(a_table) <- c("k=2", "k=5", "k=10")
my_tab <- data.frame(a_table)
my_tab
```

We see from the table that the mean and standard deviation of the error decrease as we increase the number of folds. That means that my_rf_cv becomes more accurate and more precise as we increase the number of folds. This makes sense as a greater number of folds allows for more specific predictions and fewer data points causing errors.
